{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dengue Outbreak Prediction - Real Data\n",
    "\n",
    "This notebook demonstrates dengue outbreak prediction using the **DrivenData** competition dataset.\n",
    "\n",
    "## Dataset\n",
    "- **Source**: [DrivenData - DengAI Competition](https://www.drivendata.org/competitions/44/dengai-predicting-disease-spread/)\n",
    "- **Features**: Weather data, location info\n",
    "- **Target**: Weekly dengue case counts\n",
    "\n",
    "## Approach\n",
    "1. Load and explore real dengue data\n",
    "2. Data preprocessing and quality checks\n",
    "3. Feature engineering\n",
    "4. Model training (Random Forest, XGBoost)\n",
    "5. Evaluation and outbreak detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print('Libraries loaded successfully!')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "\n",
    "Load the DrivenData dengue dataset files:\n",
    "- `dengue_features_train.csv` - Weather and environmental features\n",
    "- `dengue_labels_train.csv` - Weekly case counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load data from data/raw/dengue folder\n",
    "DATA_PATH = '../data/raw/dengue'\n",
    "\n",
    "try:\n",
    "    features_df = pd.read_csv(f'{DATA_PATH}/dengue_features_train.csv')\n",
    "    labels_df = pd.read_csv(f'{DATA_PATH}/dengue_labels_train.csv')\n",
    "    print(f'Features shape: {features_df.shape}')\n",
    "    print(f'Labels shape: {labels_df.shape}')\n",
    "except FileNotFoundError as e:\n",
    "    print('ERROR: Data files not found!')\n",
    "    print('Please download from: https://www.drivendata.org/competitions/44/dengai-predicting-disease-spread/')\n",
    "    print(f'Place files in: {DATA_PATH}/')\n",
    "    raise e"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Merge features and labels\n",
    "df = features_df.merge(labels_df, on=['city', 'year', 'weekofyear'])\n",
    "print(f'Combined dataset shape: {df.shape}')\n",
    "print(f'\\nCities: {df[\"city\"].unique()}')\n",
    "print(f'Years: {df[\"year\"].min()} - {df[\"year\"].max()}')\n",
    "\n",
    "df.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Focus on San Juan (more data available)\n",
    "city_name = 'sj'  # 'sj' for San Juan, 'iq' for Iquitos\n",
    "df = df[df['city'] == city_name].copy().reset_index(drop=True)\n",
    "print(f'Using {city_name.upper()} data: {len(df)} weeks')\n",
    "\n",
    "# Create date column\n",
    "df['date'] = pd.to_datetime(df['week_start_date'])\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "print(f'Date range: {df[\"date\"].min()} to {df[\"date\"].max()}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check columns\n",
    "print('Available columns:')\n",
    "print(df.columns.tolist())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check for missing values\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df) * 100).round(1)\n",
    "missing_df = pd.DataFrame({'Missing': missing, 'Percent': missing_pct})\n",
    "print('Missing values:')\n",
    "print(missing_df[missing_df['Missing'] > 0].sort_values('Percent', ascending=False))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Fill missing values with forward/backward fill\n",
    "df = df.ffill().bfill()\n",
    "print(f'Missing values after fill: {df.isnull().sum().sum()}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Summary statistics for target\n",
    "print('Dengue Cases Summary:')\n",
    "print(df['total_cases'].describe())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Time series plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Cases over time\n",
    "axes[0, 0].plot(df['date'], df['total_cases'], color='crimson', linewidth=0.8)\n",
    "axes[0, 0].set_title('Dengue Cases Over Time', fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Date')\n",
    "axes[0, 0].set_ylabel('Cases')\n",
    "\n",
    "# Temperature\n",
    "axes[0, 1].plot(df['date'], df['reanalysis_avg_temp_k'] - 273.15, color='orange', linewidth=0.8)\n",
    "axes[0, 1].set_title('Average Temperature (\u00b0C)', fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Date')\n",
    "axes[0, 1].set_ylabel('Temperature')\n",
    "\n",
    "# Precipitation\n",
    "axes[1, 0].bar(df['date'], df['precipitation_amt_mm'], width=5, color='steelblue', alpha=0.7)\n",
    "axes[1, 0].set_title('Precipitation (mm)', fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Date')\n",
    "axes[1, 0].set_ylabel('Precipitation')\n",
    "\n",
    "# Cases distribution\n",
    "axes[1, 1].hist(df['total_cases'], bins=50, color='green', alpha=0.7, edgecolor='black')\n",
    "axes[1, 1].set_title('Cases Distribution', fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Cases')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "Create lag features and rolling averages to capture temporal patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Rename columns for consistency\n",
    "df = df.rename(columns={\n",
    "    'reanalysis_avg_temp_k': 'temp_avg',\n",
    "    'reanalysis_min_air_temp_k': 'temp_min',\n",
    "    'reanalysis_max_air_temp_k': 'temp_max',\n",
    "    'precipitation_amt_mm': 'precipitation_mm',\n",
    "    'reanalysis_relative_humidity_percent': 'humidity_percent'\n",
    "})\n",
    "\n",
    "# Convert Kelvin to Celsius\n",
    "for col in ['temp_avg', 'temp_min', 'temp_max']:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col] - 273.15\n",
    "\n",
    "print('Columns renamed and temperature converted to Celsius')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def create_features(data):\n",
    "    '''Create lag and rolling features'''\n",
    "    df = data.copy()\n",
    "    \n",
    "    # Lag features for cases\n",
    "    for lag in [1, 2, 3, 4]:\n",
    "        df[f'cases_lag_{lag}'] = df['total_cases'].shift(lag)\n",
    "    \n",
    "    # Rolling features for weather\n",
    "    for window in [2, 4]:\n",
    "        df[f'temp_avg_roll_{window}w'] = df['temp_avg'].rolling(window).mean()\n",
    "        df[f'precip_roll_{window}w'] = df['precipitation_mm'].rolling(window).mean()\n",
    "        df[f'humidity_roll_{window}w'] = df['humidity_percent'].rolling(window).mean()\n",
    "    \n",
    "    # Seasonal encoding\n",
    "    df['week_sin'] = np.sin(2 * np.pi * df['weekofyear'] / 52)\n",
    "    df['week_cos'] = np.cos(2 * np.pi * df['weekofyear'] / 52)\n",
    "    \n",
    "    # Drop rows with NaN\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_feat = create_features(df)\n",
    "print(f'Features created: {df.shape[1]} -> {df_feat.shape[1]}')\n",
    "print(f'Samples: {len(df)} -> {len(df_feat)}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Select features for training\n",
    "exclude_cols = ['total_cases', 'date', 'week_start_date', 'city', 'year']\n",
    "feature_cols = [c for c in df_feat.columns if c not in exclude_cols \n",
    "                and df_feat[c].dtype in ['float64', 'int64', 'float32', 'int32']]\n",
    "\n",
    "print(f'Features to use: {len(feature_cols)}')\n",
    "print(feature_cols)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Prepare data\n",
    "X = df_feat[feature_cols]\n",
    "y = df_feat['total_cases']\n",
    "\n",
    "# Time-based split (no shuffle for time series)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "print(f'Training: {len(X_train)} samples')\n",
    "print(f'Testing: {len(X_test)} samples')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train Random Forest\n",
    "print('Training Random Forest...')\n",
    "rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict(X_test)\n",
    "print('Done!')\n",
    "\n",
    "# Train XGBoost\n",
    "print('\\nTraining XGBoost...')\n",
    "xgb = XGBRegressor(n_estimators=100, max_depth=6, learning_rate=0.1, random_state=42)\n",
    "xgb.fit(X_train, y_train)\n",
    "xgb_pred = xgb.predict(X_test)\n",
    "print('Done!')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def evaluate(y_true, y_pred, name):\n",
    "    '''Calculate evaluation metrics'''\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f'{name}:')\n",
    "    print(f'  MAE:  {mae:.2f}')\n",
    "    print(f'  RMSE: {rmse:.2f}')\n",
    "    print(f'  R\u00b2:   {r2:.3f}')\n",
    "    return {'MAE': mae, 'RMSE': rmse, 'R2': r2}\n",
    "\n",
    "rf_metrics = evaluate(y_test, rf_pred, 'Random Forest')\n",
    "print()\n",
    "xgb_metrics = evaluate(y_test, xgb_pred, 'XGBoost')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot predictions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].plot(y_test.values, label='Actual', color='black', linewidth=2)\n",
    "axes[0].plot(rf_pred, label='Predicted', color='crimson', alpha=0.7)\n",
    "axes[0].set_title(f'Random Forest (R\u00b2={rf_metrics[\"R2\"]:.3f})', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].set_xlabel('Test Sample')\n",
    "axes[0].set_ylabel('Cases')\n",
    "\n",
    "axes[1].plot(y_test.values, label='Actual', color='black', linewidth=2)\n",
    "axes[1].plot(xgb_pred, label='Predicted', color='steelblue', alpha=0.7)\n",
    "axes[1].set_title(f'XGBoost (R\u00b2={xgb_metrics[\"R2\"]:.3f})', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].set_xlabel('Test Sample')\n",
    "axes[1].set_ylabel('Cases')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Feature importance from XGBoost\n",
    "importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': xgb.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=importance.head(10), x='importance', y='feature', palette='viridis')\n",
    "plt.title('Top 10 Feature Importance (XGBoost)', fontweight='bold')\n",
    "plt.xlabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Top 5 features:')\n",
    "print(importance.head())"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Outbreak Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define outbreak threshold\n",
    "outbreak_threshold = y_train.quantile(0.75)\n",
    "print(f'Outbreak threshold (75th percentile): {outbreak_threshold:.1f} cases')\n",
    "\n",
    "# Evaluate detection\n",
    "actual_outbreak = y_test > outbreak_threshold\n",
    "predicted_outbreak = xgb_pred > outbreak_threshold\n",
    "\n",
    "tp = ((predicted_outbreak) & (actual_outbreak)).sum()\n",
    "fp = ((predicted_outbreak) & (~actual_outbreak)).sum()\n",
    "fn = ((~predicted_outbreak) & (actual_outbreak)).sum()\n",
    "\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "print(f'\\nOutbreak Detection:')\n",
    "print(f'  Precision: {precision:.1%}')\n",
    "print(f'  Recall: {recall:.1%}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Model\n",
    "\n",
    "Save the trained model for use by the FastAPI backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pickle\n",
    "\n",
    "# Create models folder\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Save XGBoost model\n",
    "model_path = '../models/dengue_outbreak_predictor.pkl'\n",
    "\n",
    "model_data = {\n",
    "    'model': xgb,\n",
    "    'feature_columns': feature_cols,\n",
    "    'outbreak_threshold': outbreak_threshold,\n",
    "    'metrics': xgb_metrics,\n",
    "    'data_source': 'DrivenData (San Juan)'\n",
    "}\n",
    "\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(model_data, f)\n",
    "\n",
    "print(f'Model saved to: {model_path}')\n",
    "print(f'Features: {len(feature_cols)}')\n",
    "print(f'R\u00b2 Score: {xgb_metrics[\"R2\"]:.3f}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done!\n",
    "\n",
    "Model trained and saved. Start the API:\n",
    "\n",
    "```bash\n",
    "cd ..\n",
    "uvicorn main:app --reload\n",
    "```\n",
    "\n",
    "Then visit: http://localhost:8000/docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}